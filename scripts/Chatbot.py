# recherche et gÃ©nÃ©ration des rÃ©ponses
import os
import json
import numpy as np
import faiss
import sys
import logging
from sentence_transformers import SentenceTransformer
from transformers import T5Tokenizer, T5ForConditionalGeneration

# Configuration du logging
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

# DÃ©finition des chemins
INDEX_FILE = "output/faiss_index.bin"
EMBEDDINGS_FILE = "output/embeddings.npy"
TEXTS_FILE = "output/extracted_texts.json"

# VÃ©rification de l'existence des fichiers
for file in [INDEX_FILE, EMBEDDINGS_FILE, TEXTS_FILE]:
    if not os.path.exists(file):
        logging.error(f"âŒ Erreur : Le fichier '{file}' est manquant.")
        exit(1)

# Chargement du modÃ¨le CamemBERT pour encoder les requÃªtes
logging.info("ğŸ”„ Chargement du modÃ¨le CamemBERT...")
embed_model = SentenceTransformer("camembert-base")

# Chargement du modÃ¨le T5 pour gÃ©nÃ©rer les rÃ©ponses
logging.info("ğŸ”„ Chargement du modÃ¨le T5...")
t5_tokenizer = T5Tokenizer.from_pretrained("t5-base")  # Utilisation d'un modÃ¨le plus puissant
t5_model = T5ForConditionalGeneration.from_pretrained("t5-base")

# Chargement de l'index FAISS
logging.info("ğŸ”„ Chargement de l'index FAISS...")
index = faiss.read_index(INDEX_FILE)

# Chargement des textes extraits
logging.info("ğŸ”„ Chargement des textes extraits...")
with open(TEXTS_FILE, "r", encoding="utf-8") as f:
    extracted_texts = json.load(f)

# Transformation en liste de textes
all_texts = [sentence for text in extracted_texts.values() for sentence in text.split("\n")]

# VÃ©rification de la correspondance entre FAISS et les textes
if len(all_texts) != index.ntotal:
    logging.warning(f"âš ï¸ Le nombre de textes ({len(all_texts)}) et d'embeddings dans FAISS ({index.ntotal}) ne correspond pas.")

def search_faiss(query, top_k=5):
    """Recherche les passages les plus pertinents avec FAISS."""
    logging.info(f"ğŸ” Recherche FAISS pour la requÃªte : {query}")

    query_embedding = embed_model.encode([query], normalize_embeddings=True)
    distances, indices = index.search(query_embedding, top_k)

    results = []
    for idx in indices[0]:  # Indices des passages trouvÃ©s
        if 0 <= idx < len(all_texts):
            results.append(all_texts[idx])

    logging.info(f"ğŸ“„ Passages trouvÃ©s : {results}")
    return results

def generate_answer(context, question):
    """GÃ©nÃ¨re une rÃ©ponse avec le modÃ¨le T5."""
    logging.info("ğŸ¤– GÃ©nÃ©ration de rÃ©ponse avec T5...")
    
    if not context:
        return "DÃ©solÃ©, je n'ai pas trouvÃ© d'information pertinente."

    input_text = f"question: {question} contexte: {context}"
    
    # Journaliser l'entrÃ©e envoyÃ©e Ã  T5
    logging.info(f"ğŸ“œ Texte envoyÃ© Ã  T5 : {input_text[:500]}")  # LimitÃ© Ã  500 caractÃ¨res pour Ã©viter un trop gros log

    inputs = t5_tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)
    
    outputs = t5_model.generate(
        **inputs,
        max_length=150,  # Augmentation de la longueur max
        num_beams=8,     # Plus de faisceaux pour une meilleure qualitÃ©
        temperature=0.7,  # Augmente la diversitÃ© des rÃ©ponses
        top_p=0.9,       # Nucleus sampling pour plus de variÃ©tÃ©
        early_stopping=True
    )
    
    generated_response = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
    logging.info(f"âœ… RÃ©ponse gÃ©nÃ©rÃ©e : {generated_response}")
    return generated_response

if __name__ == "__main__":
    if len(sys.argv) > 1:
        # Mode API (appelÃ© depuis Spring Boot)
        query = sys.argv[1]
        logging.info(f"ğŸ“© RequÃªte reÃ§ue : {query}")

        relevant_texts = search_faiss(query)

        # SÃ©lectionner un meilleur contexte en prenant les 3 meilleurs passages
        context = " ".join(relevant_texts[:3]) if relevant_texts else ""

        generated_response = generate_answer(context, query)

        response = {"message": generated_response}

        # ğŸ”¥ Retourne un JSON propre pour Spring Boot
        sys.stdout.reconfigure(encoding='utf-8')
        print(json.dumps(response, ensure_ascii=False))
        sys.stdout.flush()

    else:
        # Mode interactif (pour test local)
        print("\nğŸ¤– Chatbot Loi de Finances - Posez une question (tapez 'exit' pour quitter)\n")
        while True:
            query = input("Vous : ")
            if query.lower() == "exit":
                print("ğŸ‘‹ Fin de la session.")
                break

            relevant_texts = search_faiss(query)
            context = " ".join(relevant_texts[:3]) if relevant_texts else ""

            response = generate_answer(context, query)
            print(f"ğŸ¤– Chatbot : {response}")
